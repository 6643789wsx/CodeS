[
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "download_and_clean.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\ndownload_and_clean.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport argparse\nimport multiprocessing\nimport os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom itertools import repeat\nimport putiopy\nfrom libgen_to_txt.files import (\n    delete_file_locally,\n    download_folder,\n    download_folder_locally,\n    get_file_path,\n    get_parent_id,\n)\nfrom libgen_to_txt.marker.convert import process_folder_marker\nfrom libgen_to_txt.metadata import batch_write_metadata\nfrom libgen_to_txt.naive.convert import process_batch_files_naive\nfrom libgen_to_txt.settings import settings\nfrom tqdm import tqdm\n\n\ndef process_single_libgen_chunk(\n    torrent_info,\n    conversion_lock,\n    no_download,\n    no_delete,\n    max_workers=settings.CONVERSION_WORKERS,\n):\n    pass\n\n\ndef try_process_single_libgen_chunk(torrent_info, lock, options):\n    pass\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Download and process libgen\")\n    parser.add_argument(\n        \"--max\",\n        type=int,\n        default=None,\n        help=\"Maximum number of chunks to process, for testing\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=settings.DOWNLOAD_WORKERS,\n        help=\"Number of workers to use when downloading\",\n    )\n    parser.add_argument(\n        \"--no_download\",\n        action=\"store_true\",\n        help=\"Only process what already exists on the seedbox\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--no_local_delete\",\n        action=\"store_true\",\n        help=\"Do not delete files locally\",\n        default=False,\n    )\n    args = parser.parse_args()\n    os.makedirs(settings.BASE_STORAGE_FOLDER, exist_ok=True)\n    os.makedirs(settings.BASE_PROCESSED_FOLDER, exist_ok=True)\n    os.makedirs(settings.BASE_TXT_FOLDER, exist_ok=True)\n    torrent_url = \"https://libgen.rs/repository_torrent/r_{num}.torrent\"\n    nums = [str(i) for i in range(1000, 4143000, 1000)]\n    torrent_urls = []\n    for num in nums:\n        url = torrent_url.format(num=num)\n        if not os.path.exists(os.path.join(settings.BASE_PROCESSED_FOLDER, num)):\n            torrent_urls.append((num, url))\n    if args.max is not None:\n        torrent_urls = torrent_urls[: args.max]\n    m = multiprocessing.Manager()\n    lock = m.Lock()\n    args_dict = vars(args)\n    if args.workers == 1:\n        for torrent_info in tqdm(torrent_urls):\n            try_process_single_libgen_chunk(torrent_info, lock, args_dict)\n    else:\n        with ProcessPoolExecutor(max_workers=args.workers) as pool:\n            tqdm(\n                pool.map(\n                    try_process_single_libgen_chunk,\n                    torrent_urls,\n                    repeat(lock),\n                    repeat(args_dict),\n                    chunksize=1,\n                ),\n                total=len(torrent_urls),\n            )\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/files.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/files.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom typing import Optional\nfrom libgen_to_txt.settings import settings\n\n\ndef get_parent_id(client):\n    pass\n\n\ndef delete_file_locally(fpath):\n    pass\n\n\ndef get_leading_digits(s):\n    pass\n\n\ndef get_file_path(num, client, parent_id):\n    pass\n\n\ndef download_folder_locally(fpath):\n    pass\n\n\ndef download_folder(url, num, client, parent_folder_id, no_download=False):\n    pass\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/metadata.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/metadata.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport json\nimport os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom itertools import repeat\nimport pymysql\nfrom libgen_to_txt.settings import settings\n\n\ndef batch_write_metadata(files, out_folder_path, max_workers):\n    pass\n\n\ndef try_write_metadata(fmd5, out_folder_path):\n    pass\n\n\ndef write_metadata(fmd5, out_folder_path):\n    pass\n\n\ndef query_metadata(fmd5):\n    pass\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/settings.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/settings.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nfrom typing import List, Optional\nimport fitz as pymupdf\nfrom dotenv import find_dotenv\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    BASE_STORAGE_FOLDER: str = \"libgen\"\n    BASE_PROCESSED_FOLDER: str = \"processed\"\n    BASE_TXT_FOLDER: str = \"txt\"\n    BASE_METADATA_FOLDER: str = \"metadata\"\n    LIBGEN_DB_NAME: str = \"libgen\"\n    LIBGEN_DB_USER: str = \"libgen\"\n    LIBGEN_DB_PASS: str = \"password\"\n    CONVERSION_WORKERS: int = 18\n    DOWNLOAD_WORKERS: int = 8\n    MAX_TIME_TO_WAIT: int = 60 * 60 * 6\n    RCLONE_ADAPTER_NAME: str = \"putio\"\n    TEXT_FLAGS: int = pymupdf.TEXTFLAGS_TEXT & ~pymupdf.TEXT_PRESERVE_LIGATURES\n    CONVERSION_METHOD: str = \"naive\"\n    GPU_COUNT: int = 0\n    MARKER_FOLDER: str = \"../marker\"\n    MARKER_GPU_TIMEOUT: int = 60 * 60 * 8\n    MARKER_CPU_TIMEOUT: int = 60 * 60 * 24\n    MARKER_SUPPORTED_LANGUAGES: List = [\n        \"English\",\n        \"Spanish\",\n        \"Portuguese\",\n        \"French\",\n        \"German\",\n        \"Russian\",\n    ]\n    MARKER_SUPPORTED_EXTENSIONS: List = [\"pdf\", \"epub\", \"mobi\", \"xps\", \"fb2\"]\n    MARKER_MIN_LENGTH: int = 10000\n    MARKER_DEBUG_DATA_FOLDER: Optional[str] = None\n    POETRY_DIR: str = \"~/.local/bin\"\n    PUTIO_TOKEN: str = \"\"\n    PUTIO_FOLDER: str = \"libgen\"\n\n    class Config:\n        env_file = find_dotenv(\"local.env\")\n\n\nsettings = Settings()\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/util.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/util.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport magic\n\n\ndef find_filetype(fpath):\n    pass\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/marker/convert.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/marker/convert.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport json\nimport os\nimport subprocess\nimport psutil\nfrom libgen_to_txt.metadata import query_metadata\nfrom libgen_to_txt.settings import settings\n\n\ndef filter_invalid(folder_name):\n    pass\n\n\ndef wait_for_process(process, timeout, stored_path):\n    pass\n\n\ndef marker_cpu(stored_path, out_path, metadata_file, max_workers):\n    pass\n\n\ndef marker_gpu(stored_path, out_path, metadata_file, max_workers):\n    pass\n\n\ndef process_folder_marker(stored_path, out_path, num, max_workers):\n    pass\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/naive/convert.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/naive/convert.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom itertools import repeat\nfrom libgen_to_txt.naive.other import parse_djvu, parse_epub\nfrom libgen_to_txt.naive.pdf import pdf_to_text\nfrom libgen_to_txt.util import find_filetype\n\n\ndef process_single_file(fmd5, in_folder_path, out_folder_path):\n    pass\n\n\ndef try_process_single_file(fmd5, in_folder_path, out_folder_path):\n    pass\n\n\ndef process_batch_files_naive(files, stored_path, out_path, max_workers):\n    pass\n\n\ndef convert(filepath, filetype, out_filepath):\n    pass\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/naive/other.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/naive/other.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nimport subprocess\nimport ebooklib\nimport html2text as html2text\nfrom ebooklib import epub\n\n\ndef parse_epub(path):\n    pass\n\n\ndef parse_djvu(path):\n    pass\n```"
    },
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "repo_sketch": ".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py",
        "file_path": "libgen_to_txt/naive/pdf.py",
        "instruction": "Below is a detailed README.md of repository, repository sketch, as well as a file path. Please write a corresponding file sketch.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\nHere is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```\n\n---\nFile Path\n---\nlibgen_to_txt/naive/pdf.py\n\n---\nFile Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable file sketch.\n\n```python\nfrom collections import Counter\nimport fitz as pymupdf\nfrom libgen_to_txt.settings import settings\n\n\ndef filter_common_elements(lines, page_count):\n    pass\n\n\ndef filter_header_footer(page_text_blocks, max_selected_lines=2):\n    pass\n\n\ndef pdf_to_text(pdf_path: str):\n    pass\n```"
    }
]