[
    {
        "readme": "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
        "instruction": "Below is a detailed README.md of repository. Please write a repository sketch in the form of a tree, including all folders, files, as well as imports information if necessary.\n\n---\nREADME.md\n---\n# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`\n\n---\nRepository Sketch\n---\n",
        "input": "",
        "output": "Here is a practicable repository sketch.\n\n```\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 download_and_clean.py\n\u2514\u2500\u2500 libgen_to_txt\n    \u251c\u2500\u2500 files.py\n    \u251c\u2500\u2500 metadata.py\n    \u251c\u2500\u2500 settings.py\n    \u251c\u2500\u2500 util.py\n    \u251c\u2500\u2500 marker\n    |   \u2514\u2500\u2500 convert.py\n    \u2514\u2500\u2500 naive\n        \u251c\u2500\u2500 convert.py\n        \u251c\u2500\u2500 other.py\n        \u2514\u2500\u2500 pdf.py\n```"
    }
]